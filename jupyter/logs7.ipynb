{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9288bc4-317c-43c5-ab01-f1dcd32d372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from utils import detect_encoding, ip_in_subnet, my_font, generate_test_data  # 这些方法假设存在utils.py中\n",
    "from rule import AnomalyRules\n",
    "import gradio as gr\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # 设置为 DEBUG 级别以捕获调试信息\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]  # 输出到控制台\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e8ad39-cf25-4a92-a0c2-343eed95373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        初始化 ModelTrainer，设置默认路径和规则文件。\n",
    "        :param config: 配置字典，用于覆盖默认参数\n",
    "        \"\"\"\n",
    "        default_config = {\n",
    "            \"data_dir\": \"data\",\n",
    "            \"conf_dir\": \"conf\",\n",
    "            \"default_data_file\": \"default_login_logs.csv\",\n",
    "            \"default_rule_conf\": \"rule.conf\",\n",
    "            \"data_file_encoding\": \"utf-8\",\n",
    "            \"test_file_encoding\": \"utf-8\"\n",
    "        }\n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        self.data_dir = self.config[\"data_dir\"]\n",
    "        self.conf_dir = self.config[\"conf_dir\"]\n",
    "        self.default_data_file_path = os.path.join(self.data_dir, self.config[\"default_data_file\"])\n",
    "        self.default_rule_conf_path = os.path.join(self.conf_dir, self.config[\"default_rule_conf\"])\n",
    "\n",
    "        # 创建目录并检查默认文件\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        os.makedirs(self.conf_dir, exist_ok=True)\n",
    "\n",
    "        logging.debug(f\"初始化 ModelTrainer，配置：{self.config}\")\n",
    "        if not os.path.exists(self.default_data_file_path):\n",
    "            logging.warning(f\"默认数据文件 {self.default_data_file_path} 不存在。\")\n",
    "        if not os.path.exists(self.default_rule_conf_path):\n",
    "            logging.warning(f\"默认规则文件 {self.default_rule_conf_path} 不存在。\")\n",
    "\n",
    "        \n",
    "    def __init_train_conf__(self, train_data_file=None, rule_conf=None):\n",
    "        \"\"\"\n",
    "        初始化训练配置\n",
    "        \"\"\"\n",
    "        # 检查文件路径是否有效\n",
    "        logging.debug(f\"初始化训练配置，输入文件：{train_data_file}, 规则配置：{rule_conf}\")\n",
    "        if train_data_file:\n",
    "            if hasattr(train_data_file, \"name\"):  # 如果是上传的文件对象\n",
    "                self.train_data_file = train_data_file.name\n",
    "            else:\n",
    "                self.train_data_file = train_data_file\n",
    "            logging.info(f\"使用上传的训练数据文件：{self.train_data_file}\")\n",
    "        else:\n",
    "            if not os.path.exists(self.default_data_file_path):\n",
    "                raise FileNotFoundError(f\"默认数据文件 {self.default_data_file_path} 不存在，请上传文件或检查配置。\")\n",
    "            self.train_data_file = self.default_data_file_path\n",
    "            logging.info(f\"使用默认训练数据文件：{self.train_data_file}\")\n",
    "\n",
    "        # 加载规则配置\n",
    "        if rule_conf is None:\n",
    "            if not os.path.exists(self.default_rule_conf_path):\n",
    "                raise FileNotFoundError(f\"默认规则文件 {self.default_rule_conf_path} 不存在，请检查配置。\")\n",
    "            with open(self.default_rule_conf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self.rule_conf = json.load(f)\n",
    "            logging.info(f\"加载默认规则配置：{self.default_rule_conf_path}\")\n",
    "        else:\n",
    "            self.rule_conf = rule_conf\n",
    "            logging.info(\"使用上传的规则配置。\")\n",
    "    \n",
    "    def load_train_data(self, data_file_type, data_file):\n",
    "        \"\"\"\n",
    "        加载训练数据文件\n",
    "        \"\"\"\n",
    "        logging.debug(f\"加载数据文件类型：{data_file_type}, 路径：{data_file}\")\n",
    "        if data_file_type not in [\"test\", \"origin\"]:\n",
    "            raise ValueError(f\"参数 data_file_type 的值必须为 'test' 或 'origin'，而不是 '{data_file_type}'\")\n",
    "\n",
    "        encoding = detect_encoding(data_file)\n",
    "        logging.debug(f\"检测到的数据文件编码：{encoding}\")\n",
    "        df = pd.read_csv(data_file, encoding=encoding)\n",
    "        logging.info(f\"{data_file_type} 数据文件加载完成，共 {len(df)} 行记录。\")\n",
    "        logging.debug(f\"数据预览：\\n{df.head()}\")\n",
    "        return df\n",
    "    \n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"\n",
    "        数据清洗与特征提取\n",
    "        \"\"\"\n",
    "        logging.info(\"开始数据清洗和特征提取...\")\n",
    "        df[\"登录时间\"] = pd.to_datetime(df[\"登录时间\"], errors=\"coerce\")\n",
    "        df[\"登录结果\"] = df[\"登录结果\"].apply(lambda x: 1 if x == \"success\" else 0)\n",
    "        df[\"登录失败次数\"] = df.groupby(\"用户ID\")[\"登录结果\"].transform(lambda x: (x == 0).sum())\n",
    "        df[\"登录成功率\"] = df.groupby(\"用户ID\")[\"登录结果\"].transform(\"mean\")\n",
    "        df[\"时间范围分钟\"] = df.groupby(\"用户ID\")[\"登录时间\"].transform(lambda x: (x.max() - x.min()).total_seconds() / 60)\n",
    "        df[\"登录失败次数\"] = df.groupby(\"用户ID\")[\"登录结果\"].transform(lambda x: (x == 0).sum())\n",
    "        df[\"每分钟失败比例\"] = df[\"登录失败次数\"] / df[\"时间范围分钟\"]\n",
    "       \n",
    "        # 按用户ID和登录时间排序\n",
    "        df = df.sort_values(by=[\"用户ID\", \"登录时间\"])\n",
    "        # 仅保留失败登录记录\n",
    "        df[\"失败登录\"] = (df[\"登录结果\"] == 0).astype(int)\n",
    "        \n",
    "        # 计算时间间隔（秒）\n",
    "        df[\"时间间隔\"] = df.groupby(\"用户ID\")[\"登录时间\"].diff().dt.total_seconds()\n",
    "        \n",
    "        # 标记连续失败组（时间间隔大于 60 秒的视为新的组）\n",
    "        df[\"失败组\"] = (\n",
    "            (df[\"失败登录\"] == 1) & ((df[\"时间间隔\"] > 60) | (df[\"时间间隔\"].isna()))\n",
    "        ).cumsum()\n",
    "        \n",
    "        # 统计每组失败次数\n",
    "        df[\"连续失败次数\"] = df.groupby([\"用户ID\", \"失败组\"])[\"失败登录\"].transform(\"sum\")\n",
    "        \n",
    "        # 添加连续失败3次特征\n",
    "        df[\"连续失败3次\"] = (df[\"连续失败次数\"] >= 3).astype(int)\n",
    "\n",
    "        encoder = LabelEncoder()\n",
    "        df[\"用户编码\"] = encoder.fit_transform(df[\"用户ID\"])\n",
    "        logging.info(\"数据清洗完成。\")\n",
    "        logging.info(f\"清洗后的数据预览：\\n{df.head()}\")\n",
    "        return df\n",
    "    \n",
    "    def build_model(self, df):\n",
    "        \"\"\"\n",
    "        使用 Isolation Forest 构建异常检测模型\n",
    "        \"\"\"\n",
    "        logging.info(\"开始训练异常检测模型...\")\n",
    "\n",
    "        X = df.drop(['异常类型'],axis=1) # 特征集，Drop掉标签相关字段\n",
    "        y = df['异常类型']\n",
    "\n",
    "        #将数据集进行80%（训练集）和20%（验证集）的分割\n",
    "        from sklearn.model_selection import train_test_split #导入train_test_split工具\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                   test_size=0.2, random_state=0)\n",
    "\n",
    "        from sklearn.linear_model import LinearRegression # 导入线性回归算法模型\n",
    "        model = LinearRegression() # 使用线性回归算法创建模型\n",
    "\n",
    "        model.fit(X_train, y_train) # 用训练集数据，训练机器，拟合函数，确定参数\n",
    "\n",
    "        y_pred = model.predict(X_test) #预测测试集的Y值\n",
    "         \n",
    "        df_ads_pred = X_test.copy() #测试集特征数据\n",
    "        df_ads_pred['浏览量真值'] = y_test #测试集标签真值\n",
    "        df_ads_pred['浏览量预测值'] = y_pred #测试集标签预测值\n",
    "     \n",
    "        logging.info(\"模型训练完成，生成异常检测结果。\")\n",
    "        return model, df_ads_pred\n",
    "    \n",
    "    def apply_anomaly_rules(self, df):\n",
    "        anomaly_rules = AnomalyRules()\n",
    "        #内网网段\n",
    "        internal_subnets = [\"192.168.0.0/16\", \"10.0.0.0/8\", \"172.16.0.0/12\"]\n",
    "        \n",
    "        anomaly_rules.add_rule(\n",
    "            \"频繁登录失败\", \n",
    "            lambda x: x[\"连续失败次数\"] > 3,  # 失败间隔1分钟，连续失败3次\n",
    "            \"每分钟失败比例超过 0.1\"\n",
    "        )\n",
    "        # anomaly_rules.add_rule(\n",
    "        #     \"高频登录\", \n",
    "        #     lambda x: x[\"登录频率\"] > 0.5, \n",
    "        #     \"短时间内高频登录\"\n",
    "        # )\n",
    "        anomaly_rules.add_rule(\n",
    "            \"非正常源地址\",\n",
    "            lambda x: ~x[\"登录地址\"].apply(lambda ip: ip_in_subnet(ip, internal_subnets)),\n",
    "            \"登录地址属于可疑网段\"\n",
    "        )\n",
    "        anomaly_rules.add_rule(\n",
    "            \"未知用户登录\", \n",
    "            lambda x: x[\"用户ID\"] == \"unknown_user\", \n",
    "            \"未知用户尝试登录\"\n",
    "        )\n",
    "        \n",
    "        df = anomaly_rules.apply_rules(df)\n",
    "        return df\n",
    "     \n",
    "    # def combine_anomaly_results(self, df, user_stats):\n",
    "    #     \"\"\"\n",
    "    #     合并模型和规则的异常检测结果\n",
    "    #     \"\"\"\n",
    "    #     # 合并必要的列，包括登录失败次数、登录成功率、平均登录间隔等\n",
    "    #     df = df.merge(user_stats[[\"用户ID\", \"登录失败次数\", \"登录成功率\", \"平均登录间隔\", \"模型异常\"]], on=\"用户ID\", how=\"left\")\n",
    "        \n",
    "    #     df[\"是否异常\"] = df.apply(\n",
    "    #         lambda row: 1 if row[\"模型异常\"] == -1 or row[\"规则异常\"] == 1 else 0, axis=1\n",
    "    #     )\n",
    "    #     logging.info(\"异常检测结果合并完成\")\n",
    "    #     return df\n",
    "\n",
    "    def visualize_anomalies(self, df):\n",
    "        \"\"\"\n",
    "        绘制按异常类型分类的统计结果\n",
    "        \"\"\"\n",
    "        # 获取总记录数\n",
    "        total_count = len(df)\n",
    "\n",
    "        # 统计异常类型数量\n",
    "        anomaly_counts = df[\"异常类型\"].value_counts()\n",
    "\n",
    "        # 统计正常登录数量\n",
    "        normal_count = len(df[df[\"是否异常\"] == 0])\n",
    "\n",
    "        # 合并正常登录和异常类型统计\n",
    "        all_counts = pd.concat([anomaly_counts, pd.Series({\"正常登录\": normal_count})])\n",
    "        all_ratios = all_counts / total_count * 100\n",
    "\n",
    "        # 绘制饼图\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.pie(\n",
    "            all_counts,\n",
    "            labels=[f\"{label} ({ratio:.1f}%)\" for label, ratio in zip(all_counts.index, all_ratios)],\n",
    "            autopct=\"%1.1f%%\",\n",
    "            startangle=90,\n",
    "            colors=plt.cm.tab20.colors[:len(all_counts)],  # 使用预定义颜色\n",
    "            textprops={\"fontproperties\": my_font}  # 设置中文字体\n",
    "        )\n",
    "        plt.title(\"登录行为分布\", fontproperties=my_font)\n",
    "\n",
    "        # 保存图表\n",
    "        plot_file_path = os.path.join(self.data_dir, \"anomalies_plot.png\")\n",
    "        plt.savefig(plot_file_path, format=\"png\")\n",
    "\n",
    "        # 显示图表（可选）\n",
    "        plt.show()\n",
    "\n",
    "        # 保存异常记录到文件\n",
    "        anomalies = df[df[\"是否异常\"] == 1]\n",
    "        anomalies_file_path = os.path.join(self.data_dir, \"anomalies.csv\")\n",
    "        anomalies.to_csv(anomalies_file_path, index=False, encoding=self.config[\"data_file_encoding\"])\n",
    "\n",
    "        logging.info(\"\\n检测到的异常登录记录：\")\n",
    "        logging.info(anomalies[[\"用户ID\", \"登录时间\", \"登录地址\", \"登录资源\", \"登录失败次数\", \"登录成功率\", \"异常类型\"]].head(100))\n",
    "\n",
    "        return anomalies_file_path, anomalies, plot_file_path\n",
    "\n",
    "       \n",
    "\n",
    "    # def evaluate_model(self, model, test_user_stats):\n",
    "    #     \"\"\"\n",
    "    #     使用测试数据评估模型\n",
    "    #     \"\"\"\n",
    "    #     features = [\"登录失败次数\", \"登录成功率\"]\n",
    "    #     X_test = test_user_stats[features]\n",
    "\n",
    "    #     # 使用模型进行预测\n",
    "    #     test_user_stats[\"模型预测\"] = model.predict(X_test)\n",
    "    #     test_user_stats[\"模型异常\"] = test_user_stats[\"模型预测\"]  # 将模型预测结果映射为模型异常列\n",
    "\n",
    "    #     # 计算准确率、召回率、F1等指标\n",
    "    #     y_true = test_user_stats[\"模型异常\"]\n",
    "    #     y_pred = test_user_stats[\"模型预测\"]\n",
    "\n",
    "    #     # 输出分类报告\n",
    "    #     report = classification_report(y_true, y_pred, target_names=[\"正常\", \"异常\"])\n",
    "    #     cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "    #     logging.info(f\"模型评估报告：\\n{report}\")\n",
    "    #     logging.info(f\"混淆矩阵：\\n{cm}\")\n",
    "    #     return report, cm\n",
    "    \n",
    "    def optimize_model(self, X_train, y_train, model):\n",
    "        \"\"\"\n",
    "        使用网格搜索对模型进行优化\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            \"contamination\": [0.05, 0.1, 0.15, 0.2],  # 异常比例\n",
    "            \"n_estimators\": [50, 100, 200],           # 决策树数量\n",
    "            \"max_samples\": [0.8, 1.0]                  # 训练样本比例\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\", verbose=2)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # 输出最优参数\n",
    "        logging.info(f\"最优参数：{grid_search.best_params_}\")\n",
    "        logging.info(f\"最佳模型得分：{grid_search.best_score_}\")\n",
    "\n",
    "        return grid_search.best_estimator_\n",
    " \n",
    "    def train(self,uploaded_file=None, rules_input=None):\n",
    "        \n",
    "        logging.info(\"开始训练流程...\")\n",
    "        #0. 初始化数据文件和规则\n",
    "        self.__init_train_conf__(uploaded_file, rules_input)\n",
    "        #1.数据收集file-原始审计数据-选择1天的行为审计数据-格式化为csv文件-外部程序实现 access_login_data.csv\n",
    "        #2.数据可视化查看数据展示数据关系-略\n",
    "        #3.数据清洗- 根据模型定义问题处理\n",
    "           #3.1 根据编码读取数据\n",
    "        df = self.load_train_data(data_file_type=\"origin\", data_file=self.train_data_file)\n",
    "           #3.2 数据清洗和特征提取\n",
    "        pre_df = self.preprocess_data(df)\n",
    "        #4. 构建特征集和标签集（监督学习需要标签集）\n",
    "        tag_df = self.apply_anomaly_rules(pre_df)\n",
    "        \n",
    "        anomalies_file_path, anomalies, plot_file_path = self.visualize_anomalies(tag_df)\n",
    "        \n",
    "        summar_anomalie = anomalies[[\"用户ID\", \"登录地址\", \"登录失败次数\", \"登录成功率\", \"平均登录间隔\", \"异常类型\"]].head(100)\n",
    "        \n",
    "        #5. 选择算法并建立模型、训练模型-返回模型和预测结果\n",
    "        model, user_stats = self.build_model(tag_df)\n",
    "        #6. 模型优化\n",
    "           #6.1 使用规则检测异常\n",
    "        # rule_pd = self.apply_anomaly_rules(user_stats)\n",
    "           #6.2 合并模型和规则结果\n",
    "        #com_df = self.combine_anomaly_results(pre_df, rule_pd)\n",
    "           #6.3 规则可视化并保存到文件\n",
    "        \n",
    "    \n",
    "        #7.模型保存\n",
    "        model_file = \"anomaly_detection_model.pkl\"\n",
    "        import joblib\n",
    "        joblib.dump(model, model_file)\n",
    "        logging.info(f\"模型已保存到 {model_file}\")\n",
    "        \n",
    "        # #8. 生成测试集 \n",
    "        # generate_test_data(file_path=\"user_login_test_data.csv\")\n",
    "        # #9. 加载测试数据\n",
    "        # test_df = self.load_train_data(data_file_type=\"test\",data_file=\"user_login_test_data.csv\")\n",
    "        # #10. 使用训练好的模型对测试数据生成预测结果\n",
    "        # features = [\"登录失败次数\", \"登录成功率\", \"平均登录间隔\", \"活跃小时数\"]\n",
    "        # X_test = test_df[features]\n",
    "        # test_df[\"模型异常\"] = model.predict(X_test)  # 使用模型生成预测结果\n",
    "        \n",
    "        # self.evaluate_model(model, test_df)\n",
    "        \n",
    "        # #11. 优化模型\n",
    "        # features = [\"登录失败次数\", \"登录成功率\", \"平均登录间隔\", \"活跃小时数\"]\n",
    "        # X_train = user_stats[features]\n",
    "        # y_train = user_stats[\"模型异常\"]\n",
    "        \n",
    "        # optimized_model = self.optimize_model(X_train, y_train, model)\n",
    "        # # 保存优化后的模型\n",
    "        # optimized_model_file = \"optimized_anomaly_detection_model.pkl\"\n",
    "        # joblib.dump(optimized_model, optimized_model_file)\n",
    "        # logging.info(f\"优化后的模型已保存到 {optimized_model_file}\")\n",
    "        \n",
    "        return anomalies_file_path, summar_anomalie, plot_file_path, model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6380cfdb-5a0d-4f10-bc9f-cf26a82465ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_encoding, ip_in_subnet, my_font, generate_test_data  \u001b[38;5;66;03m# 这些方法假设存在utils.py中\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnomalyRules\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 主程序\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"启动训练流程...\")\n",
    "    try:\n",
    "        model = ModelTrainer()\n",
    "        anomalies_file_path, summar_anomalie, plot_file_path, model_file = model.train()\n",
    "        logging.info(f\"训练完成！\\n异常文件路径: {anomalies_file_path}\\n图表路径: {plot_file_path}\\n模型路径: {model_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"训练过程中发生错误: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98433247-a85a-4a39-930d-67b378d5f68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
