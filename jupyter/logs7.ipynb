{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9288bc4-317c-43c5-ab01-f1dcd32d372d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/conda/envs/logs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-27 11:55:16,716 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,717 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,730 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,730 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,766 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,767 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,780 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,781 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,794 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,794 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,808 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,808 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,823 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,824 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,842 - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-11-27 11:55:16,843 - DEBUG - load_verify_locations cafile='/usr/lib/ssl/cert.pem'\n",
      "2024-11-27 11:55:16,861 - DEBUG - connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=3 socket_options=None\n",
      "2024-11-27 11:55:16,865 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x796e10376440>\n",
      "2024-11-27 11:55:16,870 - DEBUG - send_request_headers.started request=<Request [b'CONNECT']>\n",
      "2024-11-27 11:55:16,874 - DEBUG - send_request_headers.complete\n",
      "2024-11-27 11:55:16,874 - DEBUG - send_request_body.started request=<Request [b'CONNECT']>\n",
      "2024-11-27 11:55:16,875 - DEBUG - send_request_body.complete\n",
      "2024-11-27 11:55:16,875 - DEBUG - receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "2024-11-27 11:55:16,881 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "2024-11-27 11:55:16,881 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x796e10354ac0> server_hostname='api.gradio.app' timeout=3\n",
      "2024-11-27 11:55:16,892 - DEBUG - Importing BlpImagePlugin\n",
      "2024-11-27 11:55:16,893 - DEBUG - Importing BmpImagePlugin\n",
      "2024-11-27 11:55:16,893 - DEBUG - Importing BufrStubImagePlugin\n",
      "2024-11-27 11:55:16,894 - DEBUG - Importing CurImagePlugin\n",
      "2024-11-27 11:55:16,894 - DEBUG - Importing DcxImagePlugin\n",
      "2024-11-27 11:55:16,895 - DEBUG - Importing DdsImagePlugin\n",
      "2024-11-27 11:55:16,897 - DEBUG - Importing EpsImagePlugin\n",
      "2024-11-27 11:55:16,897 - DEBUG - Importing FitsImagePlugin\n",
      "2024-11-27 11:55:16,898 - DEBUG - Importing FliImagePlugin\n",
      "2024-11-27 11:55:16,898 - DEBUG - Importing FpxImagePlugin\n",
      "2024-11-27 11:55:16,899 - DEBUG - Image: failed to import FpxImagePlugin: No module named 'olefile'\n",
      "2024-11-27 11:55:16,899 - DEBUG - Importing FtexImagePlugin\n",
      "2024-11-27 11:55:16,900 - DEBUG - Importing GbrImagePlugin\n",
      "2024-11-27 11:55:16,900 - DEBUG - Importing GifImagePlugin\n",
      "2024-11-27 11:55:16,901 - DEBUG - Importing GribStubImagePlugin\n",
      "2024-11-27 11:55:16,902 - DEBUG - Importing Hdf5StubImagePlugin\n",
      "2024-11-27 11:55:16,902 - DEBUG - Importing IcnsImagePlugin\n",
      "2024-11-27 11:55:16,903 - DEBUG - Importing IcoImagePlugin\n",
      "2024-11-27 11:55:16,903 - DEBUG - Importing ImImagePlugin\n",
      "2024-11-27 11:55:16,904 - DEBUG - Importing ImtImagePlugin\n",
      "2024-11-27 11:55:16,904 - DEBUG - Importing IptcImagePlugin\n",
      "2024-11-27 11:55:16,905 - DEBUG - Importing JpegImagePlugin\n",
      "2024-11-27 11:55:16,905 - DEBUG - Importing Jpeg2KImagePlugin\n",
      "2024-11-27 11:55:16,906 - DEBUG - Importing McIdasImagePlugin\n",
      "2024-11-27 11:55:16,906 - DEBUG - Importing MicImagePlugin\n",
      "2024-11-27 11:55:16,907 - DEBUG - Image: failed to import MicImagePlugin: No module named 'olefile'\n",
      "2024-11-27 11:55:16,907 - DEBUG - Importing MpegImagePlugin\n",
      "2024-11-27 11:55:16,907 - DEBUG - Importing MpoImagePlugin\n",
      "2024-11-27 11:55:16,908 - DEBUG - Importing MspImagePlugin\n",
      "2024-11-27 11:55:16,909 - DEBUG - Importing PalmImagePlugin\n",
      "2024-11-27 11:55:16,909 - DEBUG - Importing PcdImagePlugin\n",
      "2024-11-27 11:55:16,910 - DEBUG - Importing PcxImagePlugin\n",
      "2024-11-27 11:55:16,910 - DEBUG - Importing PdfImagePlugin\n",
      "2024-11-27 11:55:16,913 - DEBUG - Importing PixarImagePlugin\n",
      "2024-11-27 11:55:16,914 - DEBUG - Importing PngImagePlugin\n",
      "2024-11-27 11:55:16,914 - DEBUG - Importing PpmImagePlugin\n",
      "2024-11-27 11:55:16,915 - DEBUG - Importing PsdImagePlugin\n",
      "2024-11-27 11:55:16,915 - DEBUG - Importing QoiImagePlugin\n",
      "2024-11-27 11:55:16,916 - DEBUG - Importing SgiImagePlugin\n",
      "2024-11-27 11:55:16,916 - DEBUG - Importing SpiderImagePlugin\n",
      "2024-11-27 11:55:16,916 - DEBUG - Importing SunImagePlugin\n",
      "2024-11-27 11:55:16,917 - DEBUG - Importing TgaImagePlugin\n",
      "2024-11-27 11:55:16,917 - DEBUG - Importing TiffImagePlugin\n",
      "2024-11-27 11:55:16,917 - DEBUG - Importing WebPImagePlugin\n",
      "2024-11-27 11:55:16,918 - DEBUG - Importing WmfImagePlugin\n",
      "2024-11-27 11:55:16,919 - DEBUG - Importing XbmImagePlugin\n",
      "2024-11-27 11:55:16,919 - DEBUG - Importing XpmImagePlugin\n",
      "2024-11-27 11:55:16,920 - DEBUG - Importing XVThumbImagePlugin\n",
      "2024-11-27 11:55:17,906 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x796e103d6410>\n",
      "2024-11-27 11:55:17,907 - DEBUG - send_request_headers.started request=<Request [b'GET']>\n",
      "2024-11-27 11:55:17,907 - DEBUG - send_request_headers.complete\n",
      "2024-11-27 11:55:17,908 - DEBUG - send_request_body.started request=<Request [b'GET']>\n",
      "2024-11-27 11:55:17,908 - DEBUG - send_request_body.complete\n",
      "2024-11-27 11:55:17,908 - DEBUG - receive_response_headers.started request=<Request [b'GET']>\n",
      "2024-11-27 11:55:18,199 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 27 Nov 2024 11:55:18 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'3'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
      "2024-11-27 11:55:18,200 - INFO - HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n",
      "2024-11-27 11:55:18,201 - DEBUG - receive_response_body.started request=<Request [b'GET']>\n",
      "2024-11-27 11:55:18,201 - DEBUG - receive_response_body.complete\n",
      "2024-11-27 11:55:18,201 - DEBUG - response_closed.started\n",
      "2024-11-27 11:55:18,202 - DEBUG - response_closed.complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from utils import detect_encoding, ip_in_subnet, my_font, generate_test_data  # 这些方法假设存在utils.py中\n",
    "from rule import AnomalyRules\n",
    "import gradio as gr\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import datetime\n",
    "from utils import ip_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3e8ad39-cf25-4a92-a0c2-343eed95373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]  # 输出到控制台\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)  # 单独设置为 DEBUG 级别\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config=None):\n",
    "        \"\"\"\n",
    "        初始化 ModelTrainer，设置默认路径和规则文件。\n",
    "        :param config: 配置字典，用于覆盖默认参数\n",
    "        \"\"\"\n",
    "        default_config = {\n",
    "            \"data_dir\": \"data\",\n",
    "            \"conf_dir\": \"conf\",\n",
    "            \"default_data_file\": \"default_login_logs.csv\",\n",
    "            \"default_test_file\": \"test_login_logs.csv\",\n",
    "            \"default_rule_conf\": \"rule.yaml\",\n",
    "            \"data_file_encoding\": \"utf-8\",\n",
    "            #\"test_file_encoding\": \"utf-8\",\n",
    "            \"model_file\": \"anomaly_detection_model.pkl\"\n",
    "        }\n",
    "        \n",
    "        self.trani_features = [\"用户编码\", \"登录地址\", \"登录失败次数\", \"登录成功率\", \"时间范围分钟\", \"每分钟失败比例\", \"连续失败3次\"]\n",
    "        \n",
    "        #self.trani_features = [\"登录失败次数\", \"登录成功率\", \"时间范围分钟\", \"每分钟失败比例\", \"连续失败3次\"]\n",
    "        \n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        self.data_dir = self.config[\"data_dir\"]\n",
    "        self.conf_dir = self.config[\"conf_dir\"]\n",
    "        self.default_data_file_path = os.path.join(self.data_dir, self.config[\"default_data_file\"])\n",
    "        self.default_test_file_path = os.path.join(self.data_dir, self.config[\"default_test_file\"])\n",
    "        self.default_rule_conf_path = os.path.join(self.conf_dir, self.config[\"default_rule_conf\"])\n",
    "        self.model_file = self.config[\"model_file\"]\n",
    "\n",
    "        # 创建目录并检查默认文件\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        os.makedirs(self.conf_dir, exist_ok=True)\n",
    "\n",
    "        logging.debug(f\"初始化 ModelTrainer，配置：{self.config}\")\n",
    "        if not os.path.exists(self.default_data_file_path):\n",
    "            logging.warning(f\"默认数据文件 {self.default_data_file_path} 不存在。\")\n",
    "        if not os.path.exists(self.default_rule_conf_path):\n",
    "            logging.warning(f\"默认规则文件 {self.default_rule_conf_path} 不存在。\")\n",
    "\n",
    "        \n",
    "    def __init_train_conf__(self, train_data_file=None, rule_conf=None):\n",
    "        \"\"\"\n",
    "        初始化训练配置\n",
    "        \"\"\"\n",
    "        # 检查文件路径是否有效\n",
    "        logging.debug(f\"初始化训练配置，输入文件：{train_data_file}, 规则配置：{rule_conf}\")\n",
    "        if train_data_file:\n",
    "            if hasattr(train_data_file, \"name\"):  # 如果是上传的文件对象\n",
    "                self.train_data_file = train_data_file.name\n",
    "            else:\n",
    "                self.train_data_file = train_data_file\n",
    "            logging.info(f\"使用上传的训练数据文件：{self.train_data_file}\")\n",
    "        else:\n",
    "            if not os.path.exists(self.default_data_file_path):\n",
    "                raise FileNotFoundError(f\"默认数据文件 {self.default_data_file_path} 不存在，请上传文件或检查配置。\")\n",
    "            self.train_data_file = self.default_data_file_path\n",
    "            logging.info(f\"使用默认训练数据文件：{self.train_data_file}\")\n",
    "    \n",
    "    def load_train_data(self, data_file_type, data_file):\n",
    "        \"\"\"\n",
    "        加载训练数据文件\n",
    "        \"\"\"\n",
    "        logging.debug(f\"加载数据文件类型：{data_file_type}, 路径：{data_file}\")\n",
    "        if data_file_type not in [\"test\", \"origin\"]:\n",
    "            raise ValueError(f\"参数 data_file_type 的值必须为 'test' 或 'origin'，而不是 '{data_file_type}'\")\n",
    "\n",
    "        encoding = detect_encoding(data_file)\n",
    "        \n",
    "        self.config[\"data_file_encoding\"] = encoding\n",
    "        \n",
    "        logging.debug(f\"检测到的数据文件编码：{encoding}\")\n",
    "        df = pd.read_csv(data_file, encoding=encoding)\n",
    "        logging.info(f\"{data_file_type} 数据文件加载完成，共 {len(df)} 行记录。\")\n",
    "        logging.debug(f\"数据预览：\\n{df.head()}\")\n",
    "        return df\n",
    "    \n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"\n",
    "        数据清洗与特征提取\n",
    "        \"\"\"\n",
    "        logging.info(\"开始数据清洗和特征提取...\")\n",
    "\n",
    "        # 检查空值并填充\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            logging.warning(f\"原始数据中存在空值，进行填充处理。\\n空值统计:\\n{df.isnull().sum()}\")\n",
    "            df.fillna(value={\"登录时间\": pd.NaT, \"登录结果\": \"failure\", \"用户ID\": \"unknown_user\", \"登录地址\": \"0.0.0.0\"}, inplace=True)\n",
    "\n",
    "        # 转换登录时间\n",
    "        df[\"登录时间\"] = pd.to_datetime(df[\"登录时间\"], errors=\"coerce\")\n",
    "        if df[\"登录时间\"].isnull().any():\n",
    "            logging.warning(\"部分登录时间无法转换为时间格式，将填充为最早时间。\")\n",
    "            df[\"登录时间\"].fillna(df[\"登录时间\"].min(), inplace=True)\n",
    "\n",
    "        # 将登录结果转换为数值\n",
    "        df[\"登录结果\"] = df[\"登录结果\"].apply(lambda x: 1 if x == \"success\" else 0)\n",
    "        \n",
    "        \n",
    "        df[\"登录地址\"] = df[\"登录地址\"].apply(ip_to_int)\n",
    "\n",
    "        # 计算登录失败次数\n",
    "        df[\"登录失败次数\"] = df.groupby(\"用户ID\")[\"登录结果\"].transform(lambda x: (x == 0).sum())\n",
    "\n",
    "        # 计算登录成功率\n",
    "        df[\"登录成功率\"] = df.groupby(\"用户ID\")[\"登录结果\"].transform(lambda x: (x == 1).mean())\n",
    "\n",
    "        # 计算时间范围（分钟）\n",
    "        df[\"时间范围分钟\"] = df.groupby(\"用户ID\")[\"登录时间\"].transform(\n",
    "            lambda x: (x.max() - x.min()).total_seconds() / 60 if len(x.dropna()) > 1 else 0\n",
    "        )\n",
    "\n",
    "        # 每分钟失败比例\n",
    "        df[\"每分钟失败比例\"] = df[\"登录失败次数\"] / (df[\"时间范围分钟\"].replace(0, 1))  # 避免除以零\n",
    "\n",
    "        # 按用户ID和登录时间排序\n",
    "        df = df.sort_values(by=[\"用户ID\", \"登录时间\"])\n",
    "\n",
    "        # 标记失败登录\n",
    "        df[\"失败登录\"] = (df[\"登录结果\"] == 0).astype(int)\n",
    "\n",
    "        # 计算时间间隔（秒）\n",
    "        df[\"时间间隔\"] = df.groupby(\"用户ID\")[\"登录时间\"].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "        # 标记连续失败组（时间间隔大于60秒为新组）\n",
    "        df[\"失败组\"] = (\n",
    "            (df[\"失败登录\"] == 1) & ((df[\"时间间隔\"] > 60) | (df[\"时间间隔\"].isna()))\n",
    "        ).cumsum()\n",
    "\n",
    "        # 统计每组失败次数\n",
    "        df[\"连续失败次数\"] = df.groupby([\"用户ID\", \"失败组\"])[\"失败登录\"].transform(\"sum\")\n",
    "\n",
    "        # 添加连续失败3次特征\n",
    "        df[\"连续失败3次\"] = (df[\"连续失败次数\"] >= 3).astype(int)\n",
    "\n",
    "        # 对用户ID进行编码\n",
    "        encoder = LabelEncoder()\n",
    "        df[\"用户编码\"] = encoder.fit_transform(df[\"用户ID\"])\n",
    "        \n",
    "        # user_encoding = pd.get_dummies(df['用户ID'], prefix='用户编码')\n",
    "        # df = pd.concat([df, user_encoding], axis=1)\n",
    "\n",
    "        # 检查非数值列是否仍有异常值\n",
    "        non_numeric_columns = df.select_dtypes(include=[\"object\"]).columns\n",
    "        for col in non_numeric_columns:\n",
    "            unique_values = df[col].unique()\n",
    "            logging.info(f\"列 {col} 的唯一值：{unique_values[:5]} (仅显示前5个)\")\n",
    "\n",
    "        # 确保所有列无缺失值\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            logging.error(\"数据清洗后仍存在缺失值，请检查数据处理流程。\")\n",
    "            raise ValueError(\"数据清洗后仍存在缺失值。\")\n",
    "\n",
    "        logging.info(\"数据清洗完成。\")\n",
    "        logging.info(f\"清洗后的数据预览：\\n{df.head()}\")\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def build_model(self, df):\n",
    "        \"\"\"\n",
    "        使用 Isolation Forest 构建异常检测模型\n",
    "        \"\"\"\n",
    "        logging.info(\"开始训练异常检测模型...\")\n",
    "\n",
    "        X = df[self.trani_features]\n",
    "        \n",
    "        print(X)\n",
    "        \n",
    "        if \"异常类型编码\" in df.columns:\n",
    "            y = df[\"异常类型编码\"]\n",
    "        else:\n",
    "            logging.info(\"数据中缺少 '异常类型编码' 列，将跳过监督模型训练。\")\n",
    "            y = None  # 如果没有异常类型列\n",
    "\n",
    "        if X.isnull().sum().sum() > 0:\n",
    "            raise ValueError(\"特征数据中仍存在空值，请检查数据预处理流程。\")\n",
    "        \n",
    "        #将数据集进行80%（训练集）和20%（验证集）的分割\n",
    "        from sklearn.model_selection import train_test_split #导入train_test_split工具\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                   test_size=0.2, random_state=0)\n",
    "\n",
    "        #from sklearn.linear_model import LinearRegression # 导入线性回归算法模型\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        #model = LinearRegression() # 使用线性回归算法创建模型\n",
    "\n",
    "        model.fit(X_train, y_train) # 用训练集数据，训练机器，拟合函数，确定参数\n",
    "\n",
    "        y_pred = model.predict(X_test) #预测测试集的Y值\n",
    "         \n",
    "        df_ads_pred = X_test.copy() #测试集特征数据\n",
    "        df_ads_pred['浏览量真值'] = y_test #测试集标签真值\n",
    "        df_ads_pred['浏览量预测值'] = y_pred #测试集标签预测值\n",
    "     \n",
    "        logging.info(\"模型训练完成，生成异常检测结果。\")\n",
    "        # 保存异常记录到文件\n",
    "        train_result_file_path = os.path.join(self.data_dir, \"train_result.csv\")\n",
    "        df_ads_pred.to_csv(train_result_file_path, index=False, encoding=self.config[\"data_file_encoding\"])\n",
    "        \n",
    "        return model, df_ads_pred\n",
    "    \n",
    "    def apply_anomaly_rules(self, df):\n",
    "        anomaly_rules = AnomalyRules()\n",
    "        df = anomaly_rules.apply_rules(df)\n",
    "        \n",
    "        # 映射异常类型为数值\n",
    "        if \"异常类型\" in df.columns:\n",
    "            logging.info(\"将异常类型从字符串映射为数值...\")\n",
    "            # 创建映射表\n",
    "            unique_types = df[\"异常类型\"].dropna().unique()\n",
    "            logging.info(f\"异常类型列包含的唯一类型：: {unique_types}\")\n",
    "            unique_types = [t for t in unique_types if t != \"正常\"]  # 排除\"正常\"类型\n",
    "            type_to_id_map = {0: \"正常\"}  # \"正常\" 映射为 0\n",
    "            type_to_id_map.update({idx + 1: t for idx, t in enumerate(unique_types)})  # 为其他类型分配 ID\n",
    "            logging.info(f\"异常类型映射表: {type_to_id_map}\")\n",
    "            \n",
    "            # 映射异常类型\n",
    "            #df[\"异常类型编码\"] = df[\"异常类型\"].map(type_to_id_map).fillna(-1).astype(int)  # 将无法匹配的类型映射为 -1\n",
    "            df[\"异常类型编码\"] = df[\"异常类型\"].apply(lambda x: next((k for k, v in type_to_id_map.items() if v == x), -1))\n",
    "            tag_unique_types = df[\"异常类型编码\"].dropna().unique()\n",
    "            logging.info(f\"异常类型编码列包含的唯一类型：: {tag_unique_types}\")\n",
    "            \n",
    "            if '异常类型编码' in df.columns:\n",
    "                logging.info(\"异常类型编码列已成功创建。\")\n",
    "            else:\n",
    "                logging.error(\"异常类型编码列未成功创建，请检查映射逻辑。\")\n",
    "        else:\n",
    "            logging.error(\"数据中未找到 '异常类型' 列，请检查数据处理流程。\")\n",
    "        \n",
    "        return df,type_to_id_map\n",
    "     \n",
    "    # def combine_anomaly_results(self, df, user_stats):\n",
    "    #     \"\"\"\n",
    "    #     合并模型和规则的异常检测结果\n",
    "    #     \"\"\"\n",
    "    #     # 合并必要的列，包括登录失败次数、登录成功率、平均登录间隔等\n",
    "    #     df = df.merge(user_stats[[\"用户ID\", \"登录失败次数\", \"登录成功率\", \"平均登录间隔\", \"模型异常\"]], on=\"用户ID\", how=\"left\")\n",
    "        \n",
    "    #     df[\"是否异常\"] = df.apply(\n",
    "    #         lambda row: 1 if row[\"模型异常\"] == -1 or row[\"规则异常\"] == 1 else 0, axis=1\n",
    "    #     )\n",
    "    #     logging.info(\"异常检测结果合并完成\")\n",
    "    #     return df\n",
    "\n",
    "    def visualize_anomalies(self, df, tran_type):\n",
    "        \"\"\"\n",
    "        绘制按异常类型分类的统计结果\n",
    "        \"\"\"\n",
    "        logging.info(\"开始绘制异常数据分布图...\")\n",
    "        \n",
    "        # 获取总记录数\n",
    "        total_count = len(df)\n",
    "        logging.debug(f\"总记录数: {total_count}\")\n",
    "\n",
    "        # 统计异常类型数量\n",
    "        anomaly_counts = df[\"异常类型\"].value_counts()\n",
    "        logging.debug(f\"异常类型统计:\\n{anomaly_counts}\")\n",
    "\n",
    "        # 统计正常登录数量\n",
    "        #normal_count = len(df[df[\"是否异常\"] == 0])\n",
    "        normal_count = len(df[df[\"异常类型\"] == \"正常\"])\n",
    "        \n",
    "        non_normal_count = len(df) - normal_count\n",
    "        \n",
    "        logging.debug(f\"正常登录记录数: {normal_count}\")\n",
    "        \n",
    "\n",
    "        computed_total = anomaly_counts.sum()\n",
    "        if computed_total != total_count:\n",
    "            logging.warning(f\"统计结果总数 ({computed_total}) 与记录总数 ({total_count}) 不一致，请检查数据处理流程！\")\n",
    "        else:\n",
    "            logging.info(f\"统计结果总数与记录总数一致: {computed_total}\")\n",
    "        \n",
    "        # 合并正常登录和异常类型统计\n",
    "        all_ratios = non_normal_count / total_count * 100\n",
    "        logging.debug(f\"异常类型占比:\\n{all_ratios}\")\n",
    "\n",
    "        # 绘制饼图\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        explode = [0.05] * len(anomaly_counts)  # 将每一块分离一点，避免标签重叠\n",
    "        wedges, texts, autotexts = plt.pie(\n",
    "            anomaly_counts,\n",
    "            autopct=\"%1.1f%%\",\n",
    "            explode=explode,\n",
    "            startangle=90,\n",
    "            colors=plt.cm.tab20.colors[:len(anomaly_counts)],  # 使用预定义颜色\n",
    "            textprops={\"fontsize\": 10, \"fontproperties\": my_font},  # 设置文字大小和字体\n",
    "        )\n",
    "        \n",
    "        logging.info(\"饼图绘制完成。\")\n",
    "\n",
    "        # 设置图例，避免标签重叠\n",
    "        legend = plt.legend(\n",
    "            wedges,\n",
    "            anomaly_counts.index,\n",
    "            loc=\"center left\",\n",
    "            bbox_to_anchor=(1, 0.5),  # 图例放置在右侧\n",
    "            title=\"登录行为类型\",\n",
    "            fontsize=10,\n",
    "            title_fontsize=12,\n",
    "            prop=my_font\n",
    "        )\n",
    "    \n",
    "        if legend.get_title():\n",
    "            legend.get_title().set_fontproperties(my_font)\n",
    "            legend.get_title().set_fontsize(14)\n",
    "            \n",
    "        logging.info(\"图例设置完成。\")\n",
    "        \n",
    "        plt.title(\"登录行为分布\", fontproperties=my_font, fontsize=14)\n",
    "\n",
    "        # 保存图表\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        plot_file_path = os.path.join(self.data_dir, f\"anomalies_plot_{timestamp}.png\")\n",
    "        plt.savefig(plot_file_path, format=\"png\", bbox_inches=\"tight\", dpi=300)\n",
    "        \n",
    "        logging.info(f\"饼图已保存到文件: {plot_file_path}\")\n",
    "\n",
    "        # 显示图表（可选）\n",
    "        plt.show()\n",
    "\n",
    "        # 保存异常记录到文件\n",
    "        #anomalies = df[df[\"是否异常\"] == 1]\n",
    "        anomalies_file_path = os.path.join(self.data_dir, f\"anomalies_{tran_type}.csv\")\n",
    "        df.to_csv(anomalies_file_path, index=False, encoding=self.config[\"data_file_encoding\"])\n",
    "        logging.info(f\"异常记录已保存到文件: {anomalies_file_path}\")\n",
    "\n",
    "        logging.info(\"\\n检测到的异常登录记录：\")\n",
    "        logging.info(df[[\"用户ID\", \"登录时间\", \"登录地址\", \"登录资源\", \"登录失败次数\", \"登录成功率\", \"异常类型\"]].head(10))\n",
    "\n",
    "        return anomalies_file_path, df, plot_file_path\n",
    "\n",
    "     \n",
    "\n",
    "    def evaluate_model(self, model, test_user_stats):\n",
    "        \"\"\"\n",
    "        使用测试数据评估模型\n",
    "        \"\"\"\n",
    "        features = [\"登录失败次数\", \"登录成功率\"]\n",
    "        X_test = test_user_stats[features]\n",
    "\n",
    "        # 使用模型进行预测\n",
    "        test_user_stats[\"模型预测\"] = model.predict(X_test)\n",
    "        test_user_stats[\"模型异常\"] = test_user_stats[\"模型预测\"]  # 将模型预测结果映射为模型异常列\n",
    "\n",
    "        # 计算准确率、召回率、F1等指标\n",
    "        y_true = test_user_stats[\"模型异常\"]\n",
    "        y_pred = test_user_stats[\"模型预测\"]\n",
    "\n",
    "        # 输出分类报告\n",
    "        report = classification_report(y_true, y_pred, target_names=[\"正常\", \"异常\"])\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        logging.info(f\"模型评估报告：\\n{report}\")\n",
    "        logging.info(f\"混淆矩阵：\\n{cm}\")\n",
    "        return report, cm\n",
    "    \n",
    "    def optimize_model(self, X_train, y_train, model):\n",
    "        \"\"\"\n",
    "        使用网格搜索对模型进行优化\n",
    "        \"\"\"\n",
    "        param_grid = {\n",
    "            \"contamination\": [0.05, 0.1, 0.15, 0.2],  # 异常比例\n",
    "            \"n_estimators\": [50, 100, 200],           # 决策树数量\n",
    "            \"max_samples\": [0.8, 1.0]                  # 训练样本比例\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\", verbose=2)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # 输出最优参数\n",
    "        logging.info(f\"最优参数：{grid_search.best_params_}\")\n",
    "        logging.info(f\"最佳模型得分：{grid_search.best_score_}\")\n",
    "\n",
    "        return grid_search.best_estimator_\n",
    " \n",
    "    def predict(self, uploaded_file=None, anomalies_map=None):\n",
    "        \"\"\"\n",
    "        使用上传的测试数据文件和已训练模型进行预测，并将结果保存为 CSV 文件。\n",
    "\n",
    "        :param uploaded_file: 上传的测试数据文件\n",
    "        :param anomalies_map: 异常类型映射，必须是 JSON 格式的字典\n",
    "        :return: 保存预测结果的 CSV 文件路径\n",
    "        \"\"\"\n",
    "        # 参数验证\n",
    "        if not isinstance(anomalies_map, dict):\n",
    "            raise ValueError(\"参数 anomalies_map 必须是 JSON 格式的字典。\")\n",
    "\n",
    "        logging.info(\"开始预测流程...\")\n",
    "\n",
    "        # 初始化预测配置\n",
    "        if uploaded_file is None:\n",
    "            uploaded_file = self.default_test_file_path\n",
    "        self.__init_train_conf__(uploaded_file)\n",
    "\n",
    "        # 加载测试数据\n",
    "        df = self.load_train_data(data_file_type=\"test\", data_file=self.train_data_file)\n",
    "        pre_df = self.preprocess_data(df)\n",
    "        \n",
    "        # 提取特征\n",
    "        X_test = pre_df[self.trani_features]\n",
    "\n",
    "        # 加载训练好的模型\n",
    "        \n",
    "        model_file = self.model_file\n",
    "        model = joblib.load(model_file)\n",
    "        logging.info(f\"模型已加载，路径: {model_file}\")\n",
    "\n",
    "        # 使用模型进行预测\n",
    "        pre_df[\"异常类型编码\"] = model.predict(X_test)\n",
    "        \n",
    "        logging.info(\"模型预测完成。\")\n",
    "\n",
    "        pre_df[\"异常类型编码\"] = pre_df[\"异常类型编码\"].round().astype(int)\n",
    "        \n",
    "        unique_types2 = pre_df[\"异常类型编码\"].dropna().unique()\n",
    "        \n",
    "        logging.info(f\"预测异常类型编码2:{unique_types2}\")\n",
    "        \n",
    "        # 将异常类型编码映射为对应的文本描述，未匹配的编码显示为 \"未知\"\n",
    "        pre_df[\"异常类型\"] = pre_df[\"异常类型编码\"].apply(\n",
    "            lambda x: anomalies_map.get(str(x), \"未知\")\n",
    "        )\n",
    "        \n",
    "        unique_types2 = pre_df[\"异常类型\"].dropna().unique()\n",
    "        \n",
    "        logging.info(f\"预测异常类型2:{unique_types2}\")\n",
    "        \n",
    "        logging.info(\"异常类型映射完成。\")\n",
    "        \n",
    "        summary_predict = pre_df.head(10)\n",
    "        \n",
    "        anomalies_file_path, anomalies, plot_file_path = self.visualize_anomalies(pre_df, \"pre\")\n",
    "\n",
    "        # 保存预测结果到 CSV 文件\n",
    "        output_file = \"predicted_results.csv\"\n",
    "        pre_df.to_csv(output_file, index=False, encoding=self.config[\"data_file_encoding\"])\n",
    "        logging.info(f\"预测结果已保存到 {output_file}\")\n",
    "\n",
    "        return output_file, summary_predict, plot_file_path\n",
    "\n",
    "        \n",
    "          \n",
    "    def train(self, uploaded_file=None, rules_input=None):\n",
    "        \"\"\"\n",
    "        执行训练流程\n",
    "        :param uploaded_file: 上传的 CSV 数据文件\n",
    "        :param rules_input: 输入的 JSON 格式规则配置\n",
    "        :return: 文件路径、摘要、图表路径、模型路径、异常类型映射字典\n",
    "        \"\"\"\n",
    "        logging.info(\"开始训练流程...\")\n",
    "        try:\n",
    "            # 1. 初始化数据文件和规则\n",
    "            self.__init_train_conf__(uploaded_file, rules_input)\n",
    "            logging.info(\"训练配置初始化完成。\")\n",
    "\n",
    "            # 2. 数据加载\n",
    "            df = self.load_train_data(data_file_type=\"origin\", data_file=self.train_data_file)\n",
    "            logging.info(f\"数据加载完成，共 {len(df)} 行记录。\")\n",
    "\n",
    "            # 3. 数据清洗和特征提取\n",
    "            pre_df = self.preprocess_data(df)\n",
    "            logging.info(\"数据清洗和特征提取完成。\")\n",
    "\n",
    "            # 4. 应用异常检测规则并生成类型映射\n",
    "            tag_df, type_to_id_map = self.apply_anomaly_rules(pre_df)\n",
    "\n",
    "\n",
    "            # 校验映射结果\n",
    "            if not isinstance(type_to_id_map, dict) or not type_to_id_map:\n",
    "                raise ValueError(\"异常类型映射 (type_to_id_map) 格式不正确或为空，请检查规则配置。\")\n",
    "            logging.info(f\"异常类型映射生成完成：{type_to_id_map}\")\n",
    "\n",
    "            # 5. 可视化异常数据\n",
    "            anomalies_file_path, anomalies, plot_file_path = self.visualize_anomalies(tag_df, \"train\")\n",
    "            summar_anomalie = anomalies[[\"用户ID\", \"登录时间\", \"登录地址\", \"登录资源\", \"登录结果\", \"登录失败次数\", \"登录成功率\", \"异常类型\", \"异常类型编码\"]].head(100)\n",
    "\n",
    "            logging.info(f\"异常数据文件已保存：{anomalies_file_path}\")\n",
    "            logging.info(f\"异常统计图表已保存：{plot_file_path}\")\n",
    "\n",
    "            # 6. 模型训练\n",
    "            model, user_stats = self.build_model(tag_df)\n",
    "            logging.info(\"模型训练完成。\")\n",
    "\n",
    "            # 7. 保存模型\n",
    "            model_file = self.model_file\n",
    "            joblib.dump(model, model_file)\n",
    "            logging.info(f\"模型已保存到 {model_file}\")\n",
    "\n",
    "            # 返回训练结果\n",
    "            return anomalies_file_path, summar_anomalie, plot_file_path, model_file, type_to_id_map\n",
    "\n",
    "        except Exception as e:\n",
    "            # 捕获任何异常并记录日志\n",
    "            logging.error(f\"训练过程中发生错误：{str(e)}\", exc_info=True)\n",
    "            # 返回空值或错误提示，确保返回的值数量一致\n",
    "            return None, f\"训练失败：{str(e)}\", None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6380cfdb-5a0d-4f10-bc9f-cf26a82465ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 11:55:31,795 - INFO - 启动训练流程...\n",
      "2024-11-27 11:55:31,796 - DEBUG - 初始化 ModelTrainer，配置：{'data_dir': 'data', 'conf_dir': 'conf', 'default_data_file': 'default_login_logs.csv', 'default_test_file': 'test_login_logs.csv', 'default_rule_conf': 'rule.yaml', 'data_file_encoding': 'utf-8', 'model_file': 'anomaly_detection_model.pkl'}\n",
      "2024-11-27 11:55:31,797 - WARNING - 默认数据文件 data/default_login_logs.csv 不存在。\n",
      "2024-11-27 11:55:31,797 - WARNING - 默认规则文件 conf/rule.yaml 不存在。\n",
      "2024-11-27 11:55:31,797 - INFO - 开始训练流程...\n",
      "2024-11-27 11:55:31,797 - DEBUG - 初始化训练配置，输入文件：None, 规则配置：None\n",
      "2024-11-27 11:55:31,797 - ERROR - 训练过程中发生错误：默认数据文件 data/default_login_logs.csv 不存在，请上传文件或检查配置。\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1230332/2151635648.py\", line 462, in train\n",
      "    self.__init_train_conf__(uploaded_file, rules_input)\n",
      "  File \"/tmp/ipykernel_1230332/2151635648.py\", line 64, in __init_train_conf__\n",
      "    raise FileNotFoundError(f\"默认数据文件 {self.default_data_file_path} 不存在，请上传文件或检查配置。\")\n",
      "FileNotFoundError: 默认数据文件 data/default_login_logs.csv 不存在，请上传文件或检查配置。\n",
      "2024-11-27 11:55:31,798 - ERROR - 训练过程中发生错误: too many values to unpack (expected 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 主程序\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"启动训练流程...\")\n",
    "    try:\n",
    "        model = ModelTrainer()\n",
    "        anomalies_file_path, summar_anomalie, plot_file_path, model_file = model.train()\n",
    "        logging.info(f\"训练完成！\\n异常文件路径: {anomalies_file_path}\\n图表路径: {plot_file_path}\\n模型路径: {model_file}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"训练过程中发生错误: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98433247-a85a-4a39-930d-67b378d5f68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
